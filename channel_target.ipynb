{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e534d572-33b4-418a-a655-bfc727d618f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7219bd80-cafb-4b31-bb2e-ab6a6f1e3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import ACMerge_vgg_trainer, ACMerge_resnet_trainer\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plot\n",
    "import argparse\n",
    "import utils\n",
    "from torchvision.transforms import v2\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c2eb16-438b-48ae-bd8b-fd306d62d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "parser = argparse.ArgumentParser(description='trainer')\n",
    "parser.add_argument('--lr', type=float, default=0.008, help='learning rate') \n",
    "parser.add_argument('--data_dir', default='archive', help='data directory')\n",
    "parser.add_argument('--batch_size', type=int, default=128,help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='total epochs to run')\n",
    "parser.add_argument('--verbose', type=int, default=1, help='verbose')\n",
    "parser.add_argument('--log_freq', type=int, default=50, help='loss print freq')\n",
    "parser.add_argument('--eval_freq', type=int, default=1, help='eval freq')\n",
    "parser.add_argument('--device', default='cuda', help='cuda')\n",
    "trainer_args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81dc99b1-8ee8-4b6b-9da7-c0808e874920",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root=\"COCOSearch18\"\n",
    "with open(os.path.join(dataset_root,\n",
    "               'coco_search18_fixations_TP_train_split1.json'#'coco_search18_fixations_TP_train.json'\n",
    "               )) as json_file:\n",
    "    human_scanpaths_train = json.load(json_file)\n",
    "    \n",
    "with open(os.path.join(dataset_root,\n",
    "               'coco_search18_fixations_TP_validation_split1.json'#'coco_search18_fixations_TP_validation.json'\n",
    "               )) as json_file:\n",
    "    human_scanpaths_valid = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a696939-818f-4098-b80a-6564e41bc45a",
   "metadata": {},
   "source": [
    "### Given a concatenated neuron state maps, to figure out the correlation between these maps and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b54af2-7a1f-4d6b-877b-0370cbe96f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=utils.COCOSearch18(json=human_scanpaths_train,root='COCOSearch18/images')\n",
    "validation_dataset=utils.COCOSearch18(json=human_scanpaths_valid,root='COCOSearch18/images')\n",
    "training_loader=DataLoader(train_dataset,batch_size=trainer_args.batch_size,shuffle=True,num_workers=8,drop_last=True)\n",
    "validation_loader=DataLoader(validation_dataset,batch_size=200,shuffle=True,num_workers=8,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d4ec12-d60a-4483-9ed1-3dafc680233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0,Step:0,acc:0.080\n",
      "Epoch:0,Step:50,acc:0.825\n",
      "Epoch:0,Step:100,acc:0.820\n",
      "Epoch:0,Step:150,acc:0.785\n",
      "Epoch:1,Step:0,acc:0.775\n",
      "Epoch:1,Step:50,acc:0.755\n",
      "Epoch:1,Step:100,acc:0.785\n",
      "Epoch:1,Step:150,acc:0.830\n",
      "Epoch:2,Step:0,acc:0.760\n",
      "Epoch:2,Step:50,acc:0.780\n",
      "Epoch:2,Step:100,acc:0.860\n",
      "Epoch:2,Step:150,acc:0.840\n",
      "Epoch:3,Step:0,acc:0.815\n",
      "Epoch:3,Step:50,acc:0.780\n",
      "Epoch:3,Step:100,acc:0.795\n",
      "Epoch:3,Step:150,acc:0.810\n",
      "Epoch:4,Step:0,acc:0.810\n",
      "Epoch:4,Step:50,acc:0.755\n",
      "Epoch:4,Step:100,acc:0.875\n",
      "Epoch:4,Step:150,acc:0.790\n",
      "Epoch:5,Step:0,acc:0.795\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m center_y\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mround(bbox[:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mbbox[:,\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m     21\u001b[0m fix\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack((center_x,center_y),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m---> 22\u001b[0m observation\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trainer_args\u001b[38;5;241m.\u001b[39mbatch_size):\n\u001b[1;32m     24\u001b[0m     height\u001b[38;5;241m=\u001b[39mfovea_size \u001b[38;5;66;03m#fovea size\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fovea_size=224\n",
    "save_path='./models/acmerge_resnet_actor.pth'\n",
    "def tensor_to_PIL(tensor):\n",
    "    unloader = v2.ToPILImage()\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    return image\n",
    "    \n",
    "acmerge=ACMerge_resnet_trainer(weights=models.ResNet18_Weights.IMAGENET1K_V1).to(trainer_args.device)#models.VGG16_Weights.IMAGENET1K_V1\n",
    "#critic=Critic().to(trainer_args.device)\n",
    "optimizer=optim.Adam([acmerge.actor_read_out.weight],lr=trainer_args.lr)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "acc_temp=0\n",
    "\n",
    "for epoch in range(trainer_args.epochs):\n",
    "    for idx, (img, target_id, fixations, correct, bbox) in enumerate(training_loader):\n",
    "        \n",
    "        center_x=torch.round(bbox[:,0]+bbox[:,2]/2).to(torch.int)\n",
    "        center_y=torch.round(bbox[:,1]+bbox[:,3]/2).to(torch.int)\n",
    "        fix=torch.stack((center_x,center_y),dim=0).T\n",
    "        observation=[]\n",
    "        for i in range(trainer_args.batch_size):\n",
    "            height=fovea_size #fovea size\n",
    "            width=fovea_size\n",
    "            top=int(fix[i,1]-height/2)\n",
    "            left=int(fix[i,0]-width/2)\n",
    "            #print(top,left)\n",
    "            observation.append(v2.functional.crop(img[i],top,left,height,width)) #self-padding\n",
    "        sample=tensor_to_PIL(observation[0])\n",
    "        #plot.imshow(sample)\n",
    "        #plot.show()\n",
    "        observations=torch.stack(observation).to(trainer_args.device)\n",
    "        target_id=target_id.to(trainer_args.device)\n",
    "        optimizer.zero_grad()\n",
    "        output,_=acmerge(observations)\n",
    "        #print(output.shape,target_id.shape)\n",
    "        loss=loss_fn(output,target_id)+0.1*torch.norm(acmerge.actor_read_out.weight)\n",
    "        #print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            if idx%trainer_args.log_freq==0:\n",
    "\n",
    "                for valid_img, valid_target_id, _,_,valid_bbox in validation_loader:\n",
    "                    center_x=torch.round(valid_bbox[:,0]+valid_bbox[:,2]/2).to(torch.int)\n",
    "                    center_y=torch.round(valid_bbox[:,1]+valid_bbox[:,3]/2).to(torch.int)\n",
    "                    fix=torch.stack((center_x,center_y),dim=0).T\n",
    "                    valid_observation=[]\n",
    "                    for i in range(200):\n",
    "                        height=fovea_size #fovea size\n",
    "                        width=fovea_size\n",
    "                        top=int(fix[i,1]-height/2)\n",
    "                        left=int(fix[i,0]-width/2)\n",
    "                        #print(top,left)\n",
    "                        valid_observation.append(v2.functional.crop(valid_img[i],top,left,height,width)) #self-padding\n",
    "                    valid_observations=torch.stack(valid_observation).to(trainer_args.device)\n",
    "                    valid_target_id=valid_target_id.to(trainer_args.device)\n",
    "                    valid_output,_=acmerge(valid_observations)\n",
    "                    \n",
    "                    acc=torch.sum(torch.argmax(valid_output,dim=1)==valid_target_id)/200\n",
    "                    if acc>=acc_temp and epoch>=1:\n",
    "                        torch.save(acmerge.actor_read_out.weight,save_path)\n",
    "                        acc_temp=acc\n",
    "                    \n",
    "                    \n",
    "                    print(f'Epoch:{epoch},Step:{idx},acc:{acc:.3f}')\n",
    "                    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99bbd1-287f-4433-aedc-b7dce1203a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.max(acmerge.actor_read_out_fake.weight.squeeze()[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362fbd8-e72e-4b07-99ec-edec585dbf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
