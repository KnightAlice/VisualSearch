{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5c0291-c13d-4ad3-bab4-b1a4465ac399",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a2aa04-89a0-47ce-bf4b-c7bb5bae7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from environment import StaticImgEnv\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plot\n",
    "import argparse\n",
    "from ppo import PPO\n",
    "from agent import Agent\n",
    "import utils\n",
    "\n",
    "torch.set_printoptions(threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2da5ecb-49fb-4153-9053-c3e3508d80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description='trainer')\n",
    "parser.add_argument('--lr', type=float, default=0.05, help='learning rate') \n",
    "parser.add_argument('--data_dir', default='archive', help='data directory')\n",
    "parser.add_argument('--batch_size', type=int, default=16,help='batch size')\n",
    "parser.add_argument('--ppo_rollout_batch_size', type=int, default=16,help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=5, help='total epochs to run')\n",
    "parser.add_argument('--verbose', type=int, default=1, help='verbose')\n",
    "parser.add_argument('--loss_freq', type=int, default=50, help='loss print freq')\n",
    "parser.add_argument('--eval_freq', type=int, default=1, help='eval freq')\n",
    "parser.add_argument('--device', default='cuda', help='cuda')\n",
    "parser.add_argument('--critic_param_path', default='./models/critic.pth', help='pretrained critic')\n",
    "parser.add_argument('--disable_critic', default=False, help='no Critic')\n",
    "trainer_args = parser.parse_args(\"\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='env')\n",
    "parser.add_argument('--radius', type=int, default=112, help='fovea radius') \n",
    "parser.add_argument('--action_range', type=int, default=224, help='action range') \n",
    "parser.add_argument('--max_steps', type=int, default=15, help='max steps: -1 for unlimited') \n",
    "parser.add_argument('--grid_size', type=int, default=10, help='action space grid size')\n",
    "parser.add_argument('--plot_freq', type=int, default=10, help='plot trajectory freq')\n",
    "parser.add_argument('--target_num', type=int, default=18, help='how many targets to find')\n",
    "env_args = parser.parse_args(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4995a9-32ee-4421-9eda-6aaad1b37dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('resnetlstm', ResNetLSTM(\n",
      "  (resnet): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "  (lstm_map): LSTMCell(1000, 36)\n",
      "  (lstm_state): LSTMCell(1000, 1)\n",
      "  (relu): ReLU(inplace=True)\n",
      "))\n"
     ]
    }
   ],
   "source": [
    "dataset_root=\"COCOSearch18\"\n",
    "with open(os.path.join(dataset_root,\n",
    "               'coco_search18_fixations_TP_train_split1.json'#'coco_search18_fixations_TP_train.json'\n",
    "               )) as json_file:\n",
    "    human_scanpaths_train = json.load(json_file)\n",
    "    \n",
    "with open(os.path.join(dataset_root,\n",
    "               'coco_search18_fixations_TP_validation_split1.json'#'coco_search18_fixations_TP_validation.json'\n",
    "               )) as json_file:\n",
    "    human_scanpaths_valid = json.load(json_file)\n",
    "\n",
    "'''\n",
    "max=0\n",
    "for dict in human_scanpaths_train:\n",
    "    if dict['length']>=max:\n",
    "        max=dict['length']\n",
    "print(max)\n",
    "'''\n",
    "\n",
    "dataset=utils.COCOSearch18(json=human_scanpaths_train,root='COCOSearch18/images')\n",
    "training_loader=DataLoader(dataset,batch_size=trainer_args.batch_size,shuffle=True,num_workers=8)\n",
    "env=StaticImgEnv(env_args=env_args)\n",
    "agent=Agent(device=trainer_args.device,critic_path=trainer_args.critic_param_path,\\\n",
    "            grid_size=env_args.grid_size,disable_critic=trainer_args.disable_critic,recog_threshold=0.5)\n",
    "ppo=PPO(agent,lr=0.01,betas=[0.9,0.999],clip_param=0.2,num_epoch=1,batch_size=trainer_args.ppo_rollout_batch_size,\\\n",
    "        value_coef=0.5,entropy_coef=0.8,drop_failed=True,vgg_backbone_fixed=False) #lr=0.01 clip=0.2 value_coef=1\n",
    "for i in agent.named_children():\n",
    "     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64fd2300-e3e6-4947-b4bf-0272d1bffcfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'state_values' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m env\u001b[38;5;241m.\u001b[39mset_data(img,target_id, bbox) \u001b[38;5;66;03m#bbox->target bbox [x,y,w,h]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     trajs_all\u001b[38;5;241m=\u001b[39m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_trajs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_traj_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m utils\u001b[38;5;241m.\u001b[39mprocess_trajs(trajs_all,gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,epistemic_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Be cautious the trajectories collected->[traj_length,batch_size,...]'''\u001b[39;00m\n",
      "File \u001b[0;32m~/VS/VSTestModel/utils.py:91\u001b[0m, in \u001b[0;36mcollect_trajs\u001b[0;34m(env, policy, max_traj_length, is_eval)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_trajs\u001b[39m(env:StaticImgEnv,\n\u001b[1;32m     84\u001b[0m                    policy,\n\u001b[1;32m     85\u001b[0m                    max_traj_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     86\u001b[0m                    is_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m#collect agent trajectories and then feed to\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m#theoritically max_traj_length should be shorter than max_steps!\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     obs\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m---> 91\u001b[0m     act_batch, log_prob, value, prob, epi_value \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43msample_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43maction_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#not implemented\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     epi_values\u001b[38;5;241m=\u001b[39m[epi_value]\n\u001b[1;32m     97\u001b[0m     rewards\u001b[38;5;241m=\u001b[39m[]\n",
      "File \u001b[0;32m~/VS/VSTestModel/utils.py:56\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(obs_t, policy, sample_action, action_mask, softmask, eps)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(obs_t, policy, sample_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     54\u001b[0m                   softmask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-12\u001b[39m):\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''return grid index'''\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     probs, values , epi_value \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobs_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_action:\n\u001b[1;32m     58\u001b[0m         m \u001b[38;5;241m=\u001b[39m Categorical(probs) \u001b[38;5;66;03m#grid sample\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/VS/VSTestModel/agent.py:58\u001b[0m, in \u001b[0;36mAgent.forward\u001b[0;34m(self, x, target_id, act_h0, act_c0, val_h0, val_c0)\u001b[0m\n\u001b[1;32m     55\u001b[0m act_h,act_c,val_h,val_c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnetlstm(x,act_h0,act_c0,val_h0,val_c0)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_critic\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     state_values\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mstate_values\u001b[49m,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#pretrained to recognize categories, adding softmax to score the output\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     target_vec\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mone_hot(target_id,num_classes\u001b[38;5;241m=\u001b[39mstate_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m#target->one hot vector\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     target_values\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum(state_values\u001b[38;5;241m*\u001b[39mtarget_vec,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#element_wise mul->prob\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'state_values' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#for child in agent.critic.vgg.features:\n",
    "#    print('ReLU' in str(child))\n",
    "#raise ValueError\n",
    "import time\n",
    "def tensor_to_PIL(tensor):\n",
    "    unloader = v2.ToPILImage()\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    return image\n",
    "start=time.time()\n",
    "for idx, (img, target_id, fixations, correct, bbox) in enumerate(training_loader):\n",
    "    env.set_data(img,target_id, bbox) #bbox->target bbox [x,y,w,h]\n",
    "    with torch.no_grad():\n",
    "        trajs_all=utils.collect_trajs(env,agent,max_traj_length=env_args.max_steps)\n",
    "    utils.process_trajs(trajs_all,gamma=0.9,epistemic_coef=2)\n",
    "    '''Be cautious the trajectories collected->[traj_length,batch_size,...]'''\n",
    "    #for item in trajs_all:\n",
    "    #    print(item,trajs_all[item].shape)\n",
    "    rollouts=utils.RolloutStorage(trajs_all)\n",
    "\n",
    "    loss=ppo.update(rollouts)\n",
    "\n",
    "    #for i, sample in enumerate(data_generator):\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b4ceb-f1d5-450a-a5b2-75b9cdd9e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "def tensor_to_PIL(tensor):\n",
    "    unloader = v2.ToPILImage()\n",
    "\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    return image\n",
    "bg=torch.zeros((1,3,224,224))\n",
    "from torchvision.transforms import v2\n",
    "#top: int, left: int, height: int, width: int\n",
    "img=v2.functional.crop(bg,0,2,100,100)\n",
    "img=tensor_to_PIL(img)\n",
    "#img.show()\n",
    "draw = ImageDraw.Draw(img)\n",
    "fixations=[[1,2],[20,20]]\n",
    "fixations=[tuple(item) for item in fixations]\n",
    "print(fixations)\n",
    "draw.point(fixations,fill='red')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84e124-9cfd-47a4-9b59-a9d48dced8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal,Categorical\n",
    "import torch.nn.functional as F\n",
    "act_batch=torch.randint(-100,100,(64,2))\n",
    "#m = MultivariateNormal(act_batch, torch.eye(2))\n",
    "#m.sample()\n",
    "logits=F.softmax(abs(torch.randn(5,225)+1),dim=-1)\n",
    "print(logits)\n",
    "mvn = Categorical(logits)\n",
    "act_batch=mvn.sample()\n",
    "print(act_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d38bb-564a-47f4-a99e-6161f5484dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "a=torch.rand((14,64,18))\n",
    "b=torch.rand((14,64,18))\n",
    "entropy=nn.BCELoss(reduction='none')\n",
    "#print(a,b)\n",
    "c=torch.concatenate((a[:32],b[32:]),dim=0)\n",
    "print(entropy(b,c).mean(-1).shape)\n",
    "\n",
    "d=torch.tensor([0,1.0,0])\n",
    "e=torch.tensor([1.0,0,1.0])\n",
    "#print(entropy(d,e).mean(-1))\n",
    "# tensor(0.9964)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
