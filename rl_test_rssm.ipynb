{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5c0291-c13d-4ad3-bab4-b1a4465ac399",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3a2aa04-89a0-47ce-bf4b-c7bb5bae7624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from environment import StaticImgEnv\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plot\n",
    "import argparse\n",
    "from ppo import PPO\n",
    "from agent_rssm import Agent\n",
    "import utils\n",
    "import yaml\n",
    "from torchvision.transforms import functional\n",
    "\n",
    "torch.set_printoptions(threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2da5ecb-49fb-4153-9053-c3e3508d80dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Environment': {'radius': 112, 'vision_radius': 500, 'action_range': 224, 'max_steps': 15, 'grid_size': 7, 'plot_freq': 10, 'target_num': 18}, 'PrefixResnet': {'depth': 6, 'kernel_size': '([3,3],[3,3],[3,3])', 'in_channel': 3, 'requires_grad': False}, 'PrefixCNN': {'img_size': 224, 'patch_size': 16, 'in_channel': 3, 'out_channel': 512, 'requires_grad': True, 'pretrained_model_path': ''}, 'ViTEncoder': {'embed_dim': 512, 'depth': 24, 'num_heads': 16, 'mlp_ratio': 4.0, 'qkv_bias': False, 'qk_scale': False, 'drop_rate': 0.2, 'attn_drop_rate': 0.1, 'drop_path_rate': 0.1, 'num_classes': 100, 'weight_path': './mae_log/no_cls/encoder_param.pth'}, 'ViTDecoder': {'embed_dim': 256, 'depth': 8, 'num_heads': 16, 'mlp_ratio': 4.0, 'qkv_bias': False, 'qk_scale': False, 'drop_rate': 0.0, 'attn_drop_rate': 0.1, 'drop_path_rate': 0.1}, 'ShiftTransformer': {'embed_dim': 513, 'depth': 2, 'num_heads': 16, 'mlp_ratio': 4.0, 'qkv_bias': False, 'qk_scale': False, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.0, 'weight_path': './mae_log/shift_test/shift_param_r_1.pth'}}\n"
     ]
    }
   ],
   "source": [
    "def read_yaml_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "config = read_yaml_config('./config_shift.yaml')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "\n",
    "parser = argparse.ArgumentParser(description='trainer')\n",
    "parser.add_argument('--lr', type=float, default=0.05, help='learning rate') \n",
    "parser.add_argument('--data_dir', default='archive', help='data directory')\n",
    "parser.add_argument('--batch_size', type=int, default=16,help='batch size')\n",
    "parser.add_argument('--ppo_rollout_batch_size', type=int, default=16,help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=10, help='total epochs to run')\n",
    "parser.add_argument('--verbose', type=int, default=1, help='verbose')\n",
    "parser.add_argument('--loss_freq', type=int, default=50, help='loss print freq')\n",
    "parser.add_argument('--eval_freq', type=int, default=1, help='eval freq')\n",
    "parser.add_argument('--device', default='cuda', help='cuda')\n",
    "parser.add_argument('--critic_param_path', default='./models/acmerge_resnet_value.pth', help='pretrained critic')\n",
    "parser.add_argument('--actor_param_path', default='./models/acmerge_resnet_actor.pth', help='pretrained actor')\n",
    "parser.add_argument('--disable_critic', default=False, help='no Critic')\n",
    "trainer_args = parser.parse_args(\"\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='env')\n",
    "parser.add_argument('--radius', type=int, default=112, help='fovea radius') \n",
    "parser.add_argument('--action_range', type=int, default=224, help='action range') \n",
    "parser.add_argument('--max_steps', type=int, default=15, help='max steps: -1 for unlimited') \n",
    "parser.add_argument('--grid_size', type=int, default=7, help='action space grid size')\n",
    "parser.add_argument('--plot_freq', type=int, default=10, help='plot trajectory freq')\n",
    "parser.add_argument('--target_num', type=int, default=18, help='how many targets to find')\n",
    "env_args = parser.parse_args(\"\")\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4995a9-32ee-4421-9eda-6aaad1b37dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "dataset_root=\"COCOSearch18\"\n",
    "with open(os.path.join(dataset_root,\n",
    "               'coco_search18_fixations_TP_train_split1.json'#'coco_search18_fixations_TP_train.json'\n",
    "               )) as json_file:\n",
    "    human_scanpaths_train = json.load(json_file)\n",
    "    \n",
    "with open(os.path.join(dataset_root,\n",
    "               'coco_search18_fixations_TP_validation_split1.json'#'coco_search18_fixations_TP_validation.json'\n",
    "               )) as json_file:\n",
    "    human_scanpaths_valid = json.load(json_file)\n",
    "\n",
    "'''\n",
    "max=0\n",
    "for dict in human_scanpaths_train:\n",
    "    if dict['length']>=max:\n",
    "        max=dict['length']\n",
    "print(max)\n",
    "'''\n",
    "\n",
    "dataset=utils.COCOSearch18(json=human_scanpaths_train,root='COCOSearch18/images',test=True)\n",
    "training_loader=DataLoader(dataset,batch_size=trainer_args.batch_size,shuffle=True,num_workers=8)\n",
    "env=StaticImgEnv(env_args=config['Environment'], show_plot = True)\n",
    "agent=Agent(device=trainer_args.device, config=config,\\\n",
    "            grid_size=config['Environment']['grid_size'],disable_critic=trainer_args.disable_critic,recog_threshold=0.5)\n",
    "\n",
    "#ppo=PPO(agent,lr=0.01,betas=[0.9,0.999],clip_param=0.2,num_epoch=1,batch_size=trainer_args.ppo_rollout_batch_size,\\\n",
    "#        value_coef=0.5,entropy_coef=0.8,drop_failed=False,vgg_backbone_fixed=True) #lr=0.01 clip=0.2 value_coef=1\n",
    "\n",
    "#for i in agent.named_children():\n",
    "#     print(i)\n",
    "\n",
    "#print(agent.acmerge.value_read_out.weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9cac7f-12d6-466c-a65e-b4ed851efd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(agent.mae_encoder.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64fd2300-e3e6-4947-b4bf-0272d1bffcfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 196, 512]) torch.Size([16, 196, 512])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m env\u001b[38;5;241m.\u001b[39mset_data(img,target_id, bbox) \u001b[38;5;66;03m#bbox->target bbox [x,y,w,h]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     trajs_all\u001b[38;5;241m=\u001b[39m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_trajs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_traj_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m utils\u001b[38;5;241m.\u001b[39mprocess_trajs(trajs_all,gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,epistemic_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Be cautious the trajectories collected->[traj_length,batch_size,...]'''\u001b[39;00m\n",
      "File \u001b[0;32m~/VS/VSTestModel/utils.py:93\u001b[0m, in \u001b[0;36mcollect_trajs\u001b[0;34m(env, policy, max_traj_length, is_eval)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_trajs\u001b[39m(env,\n\u001b[1;32m     86\u001b[0m                    policy,\n\u001b[1;32m     87\u001b[0m                    max_traj_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     88\u001b[0m                    is_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m#collect agent trajectories and then feed to\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m#theoritically max_traj_length should be shorter than max_steps!\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     obs\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mobserve()\n\u001b[0;32m---> 93\u001b[0m     act_batch, log_prob, value, prob, epi_value \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43msample_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43maction_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#not implemented\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     epi_values\u001b[38;5;241m=\u001b[39m[epi_value]\n\u001b[1;32m     99\u001b[0m     rewards\u001b[38;5;241m=\u001b[39m[]\n",
      "File \u001b[0;32m~/VS/VSTestModel/utils.py:58\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(obs_t, policy, sample_action, action_mask, softmask, eps)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(obs_t, policy, sample_action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, action_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m                   softmask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-12\u001b[39m):\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''return grid index'''\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     probs, values , epi_value \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobs_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_action:\n\u001b[1;32m     60\u001b[0m         m \u001b[38;5;241m=\u001b[39m Categorical(probs) \u001b[38;5;66;03m#grid sample\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/VS/VSTestModel/agent_rssm.py:98\u001b[0m, in \u001b[0;36mAgent.forward\u001b[0;34m(self, x, target_id, prev_state, prev_action)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(h_prior_out\u001b[38;5;241m.\u001b[39mshape, h_posterior_out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#print(h_out)\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Step 3\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03msample the action with highest FEF\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Step 4\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03maction, states -- decoder --> approximate the next obs coming up\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def tensor_to_PIL(tensor):\n",
    "    img=torch.clip(img,0.0,1.0)\n",
    "    test_img=functional.to_pil_image(img)\n",
    "\n",
    "    return test_img\n",
    "    \n",
    "start=time.time()\n",
    "for epoch in range(trainer_args.epochs):\n",
    "    for idx, (img, target_id, fixations, correct, bbox) in enumerate(training_loader):\n",
    "        env.set_data(img,target_id, bbox) #bbox->target bbox [x,y,w,h]\n",
    "        with torch.no_grad():\n",
    "            trajs_all=utils.collect_trajs(env,agent,max_traj_length=env_args.max_steps)\n",
    "        utils.process_trajs(trajs_all,gamma=0.9,epistemic_coef=2)\n",
    "        '''Be cautious the trajectories collected->[traj_length,batch_size,...]'''\n",
    "        #for item in trajs_all:\n",
    "        #    print(item,trajs_all[item].shape)\n",
    "        rollouts=utils.RolloutStorage(trajs_all)\n",
    "    \n",
    "        loss=ppo.update(rollouts)\n",
    "    \n",
    "        #for i, sample in enumerate(data_generator):\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b4ceb-f1d5-450a-a5b2-75b9cdd9e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "def tensor_to_PIL(tensor):\n",
    "    unloader = v2.ToPILImage()\n",
    "\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    return image\n",
    "bg=torch.zeros((1,3,224,224))\n",
    "from torchvision.transforms import v2\n",
    "#top: int, left: int, height: int, width: int\n",
    "img=v2.functional.crop(bg,0,2,100,100)\n",
    "img=tensor_to_PIL(img)\n",
    "#img.show()\n",
    "draw = ImageDraw.Draw(img)\n",
    "fixations=[[1,2],[20,20]]\n",
    "fixations=[tuple(item) for item in fixations]\n",
    "print(fixations)\n",
    "draw.point(fixations,fill='red')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84e124-9cfd-47a4-9b59-a9d48dced8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import MultivariateNormal,Categorical\n",
    "import torch.nn.functional as F\n",
    "act_batch=torch.randint(-100,100,(64,2))\n",
    "#m = MultivariateNormal(act_batch, torch.eye(2))\n",
    "#m.sample()\n",
    "logits=F.softmax(abs(torch.randn(5,225)+1),dim=-1)\n",
    "print(logits)\n",
    "mvn = Categorical(logits)\n",
    "act_batch=mvn.sample()\n",
    "print(act_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d38bb-564a-47f4-a99e-6161f5484dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "a=torch.rand((14,64,18))\n",
    "b=torch.rand((14,64,18))\n",
    "entropy=nn.BCELoss(reduction='none')\n",
    "#print(a,b)\n",
    "c=torch.concatenate((a[:32],b[32:]),dim=0)\n",
    "print(entropy(b,c).mean(-1).shape)\n",
    "\n",
    "d=torch.tensor([0,1.0,0])\n",
    "e=torch.tensor([1.0,0,1.0])\n",
    "#print(entropy(d,e).mean(-1))\n",
    "# tensor(0.9964)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b74a1-1c10-4a78-90d9-4270e2a263be",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.zeros((16,18,100))\n",
    "b=torch.randint(low=0,high=17,size=(16,))\n",
    "print(b)\n",
    "a[torch.arange(a.shape[0]),b].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
